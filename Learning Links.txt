Natural Lanuage Procession https://www.coursera.org/course/nlpintro
Horton Works Spark&Hadoop http://hortonworks.com/solutions/spark-at-scale/
Amp Camp http://ampcamp.berkeley.edu/
Spark Hands-on https://databricks-training.s3.amazonaws.com/index.html
Spark & Cassandra https://academy.datastax.com/resources/getting-started-apache-spark-and-cassandra?unit=2217

Spark Beginner http://www.artima.com/scalazine/articles/steps.html

http://www.bestbuy.com/site/dell-latitude-12-5-refurbished-laptop-intel-core-i5-4gb-memory-750gb-hard-drive-black/5514000.p?skuId=5514000
http://www.bestbuy.com/site/hp-15-6-laptop-amd-a6-series-4gb-memory-500gb-hard-drive-black/5606300.p?skuId=5606300
http://www.bestbuy.com/site/dell-inspiron-15-6-laptop-intel-core-i3-4gb-memory-1tb-hard-drive-black/5165000.p?skuId=5165000
http://outlet.lenovo.com/outlet_us/itemdetails/20GBS00000/445
http://shop.lenovo.com/us/en/laptops/thinkpad/11e-series/11e-3rd-gen-intel/



https://github.com/Haybu/custom-kafka-record-transformer/tree/master/src/main/java/com/jupiter/stream
https://insight.io/github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ValueToKey.java

https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.2/bk_kafka-component-guide/content/ch_configuring_kafka.html
https://github.com/patelliandrea/kafka-connect-jdbc/blob/2fb62d5707f85b784678af77cf531028d74887ff/src/main/java/io/confluent/connect/jdbc/DataConverter.java


import java.math.BigDecimal;
import java.math.RoundingMode;
import java.sql.Timestamp;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Set;

import org.apache.kafka.common.cache.Cache;
import org.apache.kafka.common.cache.LRUCache;
import org.apache.kafka.common.cache.SynchronizedCache;
import org.apache.kafka.common.config.ConfigDef;
import org.apache.kafka.common.config.ConfigException;
import org.apache.kafka.connect.connector.ConnectRecord;
import org.apache.kafka.connect.data.Field;
import org.apache.kafka.connect.data.Schema;
import org.apache.kafka.connect.data.Struct;
import org.apache.kafka.connect.errors.DataException;
import org.apache.kafka.connect.transforms.Transformation;

import com.ddmi.kafka.connect.transforms.util.Requirements;
import com.ddmi.kafka.connect.transforms.util.SchemaBuilder;
import com.ddmi.kafka.connect.transforms.util.SchemaUtil;
import com.ddmi.kafka.connect.transforms.util.SimpleConfig;

public abstract class CastBigDecimal<R extends ConnectRecord<R>> implements Transformation<R> {


	public static final String OVERVIEW_DOC = "Cast BigDecimal or Number value to a FLOAT64 or double,INT64 or long and string values";
	public static final String SPEC_CONFIG = "spec";

	List<String> mappings = null;
	private Cache<Schema, Schema> schemaUpdateCache;

	public static final ConfigDef CONFIG_DEF = new ConfigDef()
			.define(SPEC_CONFIG, ConfigDef.Type.LIST, ConfigDef.NO_DEFAULT_VALUE, new ConfigDef.Validator() {
				@SuppressWarnings("unchecked")
				@Override
				public void ensureValid(String name, Object valueObject) {
					List<String> value = (List<String>) valueObject;
					if (value == null || value.isEmpty()) {
						throw new ConfigException("Must specify at least one field to cast.");
					}
					parseFieldTypes(value);
				}

				@Override
				public String toString() {
					return "list of colon-delimited pairs, e.g. <code>foo:bar,abc:xyz</code>";
				}
			},
					ConfigDef.Importance.HIGH,
					"List of fields and the type to cast them to of the form field1:type,field2:type to cast fields of "
							+ "Individual type. A single type to cast the entire value. Valid types are double, long, string.");

	private static final String WHOLE_VALUE_CAST = null;

	private static final String PURPOSE = "cast types";

	protected abstract Object operatingValue(R record);

	protected abstract Schema operatingSchema(R record);

	protected abstract R newRecord(R record, Schema updatedSchema, Object updatedValue);

	private R applySchemaless(R record) {
		final Map<String, Object> value = requireMap(operatingValue(record), PURPOSE);
		final HashMap<String, Object> updatedValue = new HashMap<>(value);
		Map<String,Schema.Type> casts =  processSpecs(mappings);
		for (Map.Entry<String,Schema.Type> fieldSpec : casts.entrySet()) {
			String field = fieldSpec.getKey();
			updatedValue.put(field, castValueToType(value.get(field),fieldSpec.getValue()));
		}
		return newRecord(record, null, updatedValue);
	}


	Map<String,Schema.Type> casts;
	private R applyWithSchema(R record) {
		Schema valueSchema = operatingSchema(record);
		//System.out.println("******** applyWithSchema has been called *********"+record.toString());
		Schema updatedSchema = getOrBuildSchema(valueSchema);        // Casting within a struct
		final Struct value = Requirements.requireStruct(operatingValue(record), PURPOSE);

		final Struct updatedValue = new Struct(updatedSchema);
		for (Field field : value.schema().fields()) {
			//System.out.println("******** applyWithSchema for loop *********"+field.name());
			final Object origFieldValue = value.get(field);
			final Schema.Type targetType = casts.get(field.name());
			final Object newFieldValue = targetType != null ? castValueToType(origFieldValue, targetType) : origFieldValue;
			updatedValue.put(updatedSchema.field(field.name()), newFieldValue);
		}
		return newRecord(record, updatedSchema, updatedValue);
	}


	private Schema getOrBuildSchema(Schema valueSchema) {
		Schema updatedSchema = schemaUpdateCache.get(valueSchema);
		if (updatedSchema != null)
			return updatedSchema;

		final SchemaBuilder builder;
		builder = SchemaUtil.copySchemaBasics(valueSchema, SchemaBuilder.struct());
		for (Field field : valueSchema.fields()) {
			//System.out.println("Field Name "+field.name()+" : "+field.schema()+" : "+field.schema().defaultValue());
			SchemaBuilder fieldBuilder =
					convertFieldType(casts.containsKey(field.name()) ? casts.get(field.name()) : field.schema().type());
			if (field.schema().isOptional())
				fieldBuilder.optional();
			if (field.schema().defaultValue() != null)
				fieldBuilder.defaultValue(castValueToType(field.schema().defaultValue(), fieldBuilder.type()));
			builder.field(field.name(), fieldBuilder.build());
		}


		if (valueSchema.isOptional())
			builder.optional();
		if (valueSchema.defaultValue() != null)
			builder.defaultValue(castValueToType(valueSchema.defaultValue(), builder.type()));

		updatedSchema = builder.build();
		schemaUpdateCache.put(valueSchema, updatedSchema);
		return updatedSchema;
	}
	private static final Set<Schema.Type> SUPPORTED_CAST_TYPES = new HashSet<>(
			Arrays.asList(Schema.Type.INT8, Schema.Type.INT16, Schema.Type.INT32, Schema.Type.INT64,
					Schema.Type.FLOAT32, Schema.Type.FLOAT64, Schema.Type.BOOLEAN, Schema.Type.STRING)
			);

	private SchemaBuilder convertFieldType(Schema.Type type) {
		switch (type) {
		case INT8:
			return SchemaBuilder.int8();
		case INT16:
			return SchemaBuilder.int16();
		case INT32:
			return SchemaBuilder.int32();
		case INT64:
			return SchemaBuilder.int64();
		case FLOAT32:
			return SchemaBuilder.float32();
		case FLOAT64:
			return SchemaBuilder.float64();
		case BOOLEAN:
			return SchemaBuilder.bool();
		case STRING:
			return SchemaBuilder.string();
		case BYTES:
			return SchemaBuilder.bytes();
		default:
			throw new DataException("Unexpected type in Cast transformation: " + type);
		}

	}

	private static Object castValueToType(Object value, Schema.Type targetType) {
		//System.out.println("******** castValueToType has been called *********"+"   "+targetType);
		try {
			if (value == null) return null;

			switch (targetType) {
			case FLOAT64:
				return castToDouble(value);
			case INT64:
				return castToLong(value);
			default:
				throw new DataException(targetType.toString() + " is not supported in the Cast transformation.");
			}
		} catch (NumberFormatException e) {
			throw new DataException("Value (" + value.toString() + ") was out of range for requested data type", e);
		}

	}

	private static double castToDouble(Object value) {
		if (value instanceof BigDecimal) {
			BigDecimal val = ((BigDecimal) value).setScale(3, RoundingMode.HALF_UP);
			return val.doubleValue();
		}
		else if (value instanceof Number)
			return ((Number) value).doubleValue();
		else
			throw new DataException("Unexpected type in Cast transformation: " + value.getClass());
	}

	private static long castToLong(Object value) {		
		if (value instanceof BigDecimal)
			return ((BigDecimal) value).longValue();
		else if (value instanceof Number)
			return ((Number) value).longValue();
		else if (value instanceof Boolean)
			return ((boolean) value) ? (long) 1 : (long) 0;
			else if (value instanceof String)
				return Long.parseLong((String) value);
			else if(value instanceof Timestamp)
				return ((Timestamp) value).getTime();
			else
				throw new DataException("Unexpected type in Cast transformation: " + value.getClass());
	}

	private enum FieldType {
		INPUT, OUTPUT
	}


	private Map<String, Schema.Type> processSpecs(List<String> confMappings) {
		//System.out.println("******** processSpecs has been called *********" +confMappings);
		final Map<String, Schema.Type> m = new HashMap<String,Schema.Type>();

		for (String mapping : confMappings) {
			final String[] parts = mapping.split(":");
			if (parts.length > 2) {
				throw new ConfigException(SPEC_CONFIG, mappings, "Invalid spec mapping: " + mapping);
			}
			else if (parts.length < 2) {
				throw new ConfigException(SPEC_CONFIG, mappings, "Invalid spec mapping: " + mapping);
			}
			else {
				Schema.Type type;
				try {
					type = Schema.Type.valueOf(parts[1].trim().toUpperCase(Locale.ROOT));
				} catch (IllegalArgumentException e) {
					throw new ConfigException("Invalid type found in casting spec: " + parts[1].trim(), e);
				}
				m.put(parts[0].trim(), validCastType(type, FieldType.OUTPUT));
			}
		}


		return m;
	}

	private static Schema.Type validCastType(Schema.Type type, FieldType fieldType) {
		if (!SUPPORTED_CAST_TYPES.contains(type)) {
			String message = "Cast transformation does not support casting to/from " + type
					+ "; supported types are " + SUPPORTED_CAST_TYPES;
			switch (fieldType) {
			case INPUT:
				throw new DataException(message);
			case OUTPUT:
				throw new ConfigException(message);
			}
		}
		return type;
	}



	@Override
	public void configure(Map<String, ?> props) {
		final SimpleConfig config = new SimpleConfig(CONFIG_DEF, props);
		mappings = config.getList(SPEC_CONFIG);
		casts =  processSpecs(mappings);
		schemaUpdateCache = new SynchronizedCache<>(new LRUCache<Schema, Schema>(16));
	}

	@Override
	public R apply(R record) {

		Schema s = operatingSchema(record);
		if (s == null) {
			return applySchemaless(record);
		} else {
			return applyWithSchema(record);
		}
	}



	@Override
	public void close() {
	}

	@Override
	public ConfigDef config() {
		return CONFIG_DEF;
	}


	private static Map<String, Schema.Type> parseFieldTypes(List<String> mappings) {
		final Map<String, Schema.Type> m = new HashMap<String, Schema.Type>();
		boolean isWholeValueCast = false;
		for (String mapping : mappings) {
			final String[] parts = mapping.split(":");
			if (parts.length > 2) {
				throw new ConfigException(SPEC_CONFIG, mappings, "Invalid spec mapping: " + mapping);
			}
			if (parts.length == 1) {
				Schema.Type targetType = Schema.Type.valueOf(parts[0].trim().toUpperCase(Locale.ROOT));
				m.put(WHOLE_VALUE_CAST, validCastType(targetType, FieldType.OUTPUT));
				isWholeValueCast = true;
			} else {
				Schema.Type type;
				try {
					type = Schema.Type.valueOf(parts[1].trim().toUpperCase(Locale.ROOT));
				} catch (IllegalArgumentException e) {
					throw new ConfigException("Invalid type found in casting spec: " + parts[1].trim(), e);
				}
				m.put(parts[0].trim(), validCastType(type, FieldType.OUTPUT));
			}
		}
		if (isWholeValueCast && mappings.size() > 1) {
			throw new ConfigException("Cast transformations that specify a type to cast the entire value to "
					+ "may ony specify a single cast in their spec");
		}
		return m;
	}


	public static final class Value<R extends ConnectRecord<R>> extends CastBigDecimal<R> {
		@Override
		protected Schema operatingSchema(R record) {
			return record.valueSchema();
		}

		@Override
		protected Object operatingValue(R record) {
			return record.value();
		}

		@Override
		protected R newRecord(R record, Schema updatedSchema, Object updatedValue) {
			return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(), updatedSchema, updatedValue, record.timestamp());
		}
	}

}
